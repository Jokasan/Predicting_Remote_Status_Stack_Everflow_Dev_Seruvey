---
title: "Predicting Developer Remote Working Status"
date: "`r Sys.Date()`"
author: Nils Indreiten
output:
    rmdformats::robobook:
    code_folding: show
    self_contained: true
    thumbnails: false
    lightbox: true
pkgdown:
  as_is: true   
---

```{r, echo=FALSE, message=FALSE}
pacman::p_load(tidymodels,readr,vip,forcats)
stack_overflow<- read_csv("https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/stack_overflow.csv")
```


# Exploring and prepping the data

The dataset originally from the [Stack Overflow Developer 
Survey](https://insights.stackoverflow.com/survey/2021), was provided 
in [chapter 2 of Julia Sigle's Supervised Machine Learning Case
Studies in R](https://supervised-ml-course.netlify.app).  Lets explore 
the dataset, we begin by checking how many remote and non-remote 
developers there are and a breakdown by country.

```{r}
# Quick look at the dataset:
glimpse(stack_overflow)
```
```{r}
# First count for remote
stack_overflow %>% 
  count(remote, sort = TRUE)
# Then by country
stack_overflow %>% 
  count(country, sort = TRUE)
```

Lets plot a boxplot of the employment type and years of professional coding experience. 
```{r}
ggplot(stack_overflow, aes(remote,years_coded_job))+
  geom_boxplot()+
  labs(x=NULL,
       y="Yeas of professional coding experience")+
  theme_light()
```

```{r}
stack_overflow %>% 
  group_by(country) %>% 
  summarise(salary=median(salary)) %>% 
  mutate(country=fct_reorder(country,salary)) %>% 
  ggplot(aes(country,salary))+
  geom_col(fill="midnightblue")+
  theme_minimal()+
  coord_flip()+
  xlab("")+
  ylab("")+
  scale_y_continuous(labels = scales::dollar_format())+
  ggtitle("Median Salary by Country")
```

## Training and testing data

Before dealing with the imbalance in the remote/not remote classes,
we first split the data into training and test datasets dividing  the 
original data into %80/20 sections and about evenly dividing the 
sections between different classes of remote. 

```{r}
# make character variables into factors
stack_overflow <- stack_overflow %>% 
  mutate(remote=factor(remote, levels = c("Remote","Not remote"))) %>% 
  mutate_if(is.character,factor)
# Create stack_select dataset
stack_select <-  stack_overflow %>% 
  select(-respondent)
# Split the data into training and testing sets
set.seed(1234)
stack_split <- initial_split(stack_select, strata=remote)
stack_train <- training(stack_split)
stack_test <- testing(stack_split)
# Take a quick look at the training data:
glimpse(stack_train)
```

## Dealing with unmbalanced data

In order to address the class imbalance of our dataset, we can use
downsampling. 
```{r}
stack_overflow %>% 
  count(remote)
```

Downsampling removes some of the majority class so that it
has less effect on the predictive model. It also randomly 
removes examples from the majority class until it is equal
to the minority class,in size. Here we will implement
downsampling using the step_downsample() function from the
recipes package. Downsampling is performed on the training dataset:

```{r}
stack_recipe <- recipe(remote~., data = stack_train) %>% 
  step_downsample(remote)
stack_recipe
```
# Validation split
Well use the validation_split() function to allocate 20% of the remote
developers to the _validation set_ and the rest to the _training set_. 
This should provide enough precision to be a reliable indicator 
for how well each model predicts the outcome with a single 
iteration of resampling. 

```{r}
set.seed(234)
val_set <- validation_split(stack_train,
                            strata=remote,
                            prop = 0.8)
val_set
```

# A decision tree model

Build the model:

```{r}
dt_mod <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth=tune(),
    min_n = tune(),
    ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
dt_mod
```

Specify the the decision tree recipe:
```{r}
dt_recipe <- 
  stack_recipe <- recipe(remote~., data = stack_train) %>% 
  step_downsample(remote) 
```
Create the workflow:
```{r}
dt_workflow <- 
  workflow() %>% 
  add_model(dt_mod) %>% 
  add_recipe(dt_recipe)
```

## Create the grid for tuning:
```{r}
tree_grid <- grid_regular(cost_complexity(),tree_depth(), min_n(),levels=4)
tree_grid
```
Train and tune the model:
```{r}
dt_res <- 
  dt_workflow %>% 
  tune_grid(val_set,
            grid=tree_grid,
            control=control_grid(save_pred = TRUE),
            metrics=metric_set(roc_auc))
dt_res
```
Plot the ROC curve for the decision tree:
```{r}
dt_best <- 
  dt_res %>% 
  collect_metrics()
dt_best
dt_auc <- 
  dt_res %>% 
  collect_predictions(parameters=dt_best) %>% 
  roc_curve(remote,.pred_Remote) %>% 
  mutate(model="Decision Tree")
autoplot(dt_auc)
```


# A random forest model

Build the model:
```{r}
rf_mod <- 
  rand_forest(mtry=tune(),min_n=tune(), trees=1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")
```
Define the recipe :
```{r}
rf_recipe <- recipe(remote~., data = stack_train) %>% 
  step_downsample(remote)
rf_recipe
```
Create the workflow:
```{r}
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)
```

Train and tune the model:
```{r}
# show what will be tuned
rf_mod %>% 
  parameters()
```
```{r}
set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid=25,
            control=control_grid(save_pred = TRUE),
            metrics=metric_set(roc_auc))
```
Top 5 models from the 25 candidates:
```{r}
rf_res %>% 
  show_best(metric="roc_auc")
```

```{r}
autoplot(rf_res)
```

Lets select the best model according to the ROC AUC metric. Our final
values are:
```{r}
rf_best <- 
  rf_res %>% 
  select_best(metric="roc_auc")
rf_best
```
To calculate the data needed to plot ROC curve, we use 
collect_predictions().
```{r}
rf_res %>% 
  collect_predictions()
```
```{r}
rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters=rf_best) %>% 
                        roc_curve(remote,.pred_Remote) %>% 
  mutate(model="Rndom Forest")
```
# Comparing the ROC curves for our top tuned decision
and random forest models:

```{r}
bind_rows(rf_auc, dt_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)+theme_light()
```


# The last fit

```{r}
last_rf_mod <- 
  rand_forest(mtry=2, min_n=31, trees=1000) %>% 
  set_engine("ranger", importance="impurity") %>% 
  set_mode("classification")
# last workflow:
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)
# The last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(stack_split)
last_rf_fit
```
We can see the variable importance scores via the .workflow column. 
First we need to pluck out the first element in the workflow column,
then pull out the fit from the workflow object. Finally, the 
vip package helps us visualise the variable importance:

```{r}
last_rf_fit %>% 
  pluck(".workflow",1) %>% 
  pull_workflow_fit() %>% 
 vip(geom = "col", aesthetics = list(fill = "midnightblue", alpha = 0.8)) +
  scale_y_continuous(expand = c(0, 0))+
  theme_minimal()
```

Lets generate an ROC curve, for our final fitted random forest model:
```{r}
last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(remote,.pred_Remote) %>% 
  autoplot()
```

Based on these results, the validation set and test set performance
statistics are very close, so we would have pretty high confidence
that our random forest model with the selected hyperparameters
would perform as we expect on new data.

# Session Info
```{r}
sessionInfo()
```

