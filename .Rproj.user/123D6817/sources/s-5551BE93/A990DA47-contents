---
title: "Chapter 2: Stack Overflow Developer Survey"
output: html_notebook
---
```{r}
library(tidyverse)
csv_url <- "https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/stack_overflow.csv"
download.file(csv_url,"stack_overflow.csv")
stack_overflow<- readr::read_csv("stack_overflow.csv")
```

# EDA

Lets explore the dataset, kets begin by checking how many remote
and non-remote developers there are, where they live and how
experienced they are. 
```{r}
glimpse(stack_overflow)

# First count for ~remote~
stack_overflow %>% 
  count(remote, sort = TRUE)

stack_overflow %>% 
  count(country, sort = TRUE)

```

Lets plot a boxplot of the employment type and hours of work per week. 
```{r}
ggplot(stack_overflow, aes(remote,years_coded_job))+
  geom_boxplot()+
  labs(x=NULL,
       y="Yeas of professional coding experience")+
  theme_light()
```

```{r}
p1 <- stack_overflow %>% 
  group_by(country) %>% 
  summarise(salary=median(salary)) 

p1 %>% 
  mutate(country=fct_reorder(country,-salary)) %>% 
  ggplot(aes(country,salary))+
  geom_col(fill="midnightblue")+
  theme_minimal()+
  coord_flip()+
  xlab("")+
  ylab("")+
  scale_y_continuous(labels = scales::dollar_format())
  ggtitle("Median Salary by Country")
```

# Training and testing data

Before dealing with the imbalance in the remote/not remote classes,
first split your data into training and testing sets. You create
subsets of your data for training and testing your model for the 
same reasons you did before: to reduce overfitting and obtain a 
more accurate estimate of how your model will perform on new data. 

Create a data split that divides the original data into %80/20 
sections and about evenly divides the sections between different
classes of remote. 

```{r}
library(tidymodels)
stack_overflow <- stack_overflow %>% 
  mutate(remote=factor(remote, levels = c("Remote","Not remote"))) %>% 
  mutate_if(is.character,factor)
# Create stack_select dataset
stack_select <-  stack_overflow %>% 
  select(-respondent)
# Split the data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>% 
  initial_split(prop=0.8,strata = remote)
stack_train <- training(stack_split)
stack_test <- testing(stack_split)
glimpse(stack_test)
glimpse(stack_train)
```

# Dealing with imbalanced data

In order to address the class imbalance of our dataset, we can use
downsampling. 
```{r}
stack_overflow %>% 
  count(remote)
```

Downsampling removes some of the majority class so that it has
less effect on the predicitve model. It also randomly removes
examples from the majority class until it is equal to the minority class in size. Here we will implement downsampling using the
step_downsample() function from the recipes package. Downsampling
is performed on the training dataset:

```{r}
stack_recipe <- recipe(remote~., data = stack_train) %>% 
  step_downsample(remote)
stack_recipe
```

Once we have defined our recipe, you can estimate the parameters
required to actually preprocess the data, and then extract the
processed data. This typically isnt necessary if you use a
wokflow() for modelling, but it can be helpful to diagnose
problems or explore your preprocessing results. 

* First we prep() the recipe.

* Then, bake() the prepped recipe with new_data=NULL to see the
preprocessed training data. Check out the results of remote
sampling after status. 
```{r}
stack_prep <- prep(stack_recipe)
stack_down <- bake(stack_prep, new_data = NULL)
stack_down %>% 
  count(remote)
```

Baking the prepped recipe stack_prep with new_data=NULL, allows
you to extract the procesed training data.

# Train models

Finally we can train our models. We will specify ur machine 
learning models with parsnip, and use workflows for
convenience. First logistic regression model:
```{r}
# Build a iogistic regression model
glm_spec <- logistic_reg() %>% 
  set_engine("glm")
# Start a recipe workflow (recipe only)
stack_wf <- workflow()%>% 
  add_recipe(stack_recipe)
# add the model fit and workflow
stack_glm <- stack_wf %>% 
  add_model(glm_spec) %>% 
  fit(data=stack_train)

# print the fitted model:
stack_glm
```

Now a deecision tree model:
```{r}
# Buils a decision tree model
tree_spec <- decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
## start workflow (recipe only)
stack_wf <- workflow() %>% 
  add_recipe(stack_recipe)
## add the model and fit the workflow:
stack_tree <- stack_wf %>% 
  add_model(tree_spec) %>% 
  fit(stack_train)
# print the fitted model
stack_tree
```

Now a RF:
```{r}
# Builds a decision tree model
forest_spec <-rand_forest() %>% 
  set_engine("randomForest") %>% 
  set_mode("classification")
## start workflow (recipe only)
stack_wf <- workflow() %>% 
  add_recipe(stack_recipe)
## add the model and fit the workflow:
stack_forest <- stack_wf %>% 
  add_model(forest_spec) %>% 
  fit(stack_train)
# print the fitted model
stack_forest
```

# Confusion Matrix

A confusion gives us an indication of how well a classification 
model performs. In short, a confusion matrix tabulates the 
examples in each class that were correctly classified
by a model. In this case it will tell use how many remote
developers were classified as remote and how many non-remote
developers were classified as non-remote; in addition the 
confusion matrix shows the wrongly classified categories. 

We will use the conf_mat() function from yardstick to evaluate
performance of the three models that we trained, stack_glm,
stack_tree, and stack_rf. 
```{r}
results <- stack_test %>% 
  bind_cols(predict(stack_glm, stack_test) %>% 
              rename(.pred_glm=.pred_class))
# confusion matrix for logistic regression model
results %>% 
  conf_mat(truth=remote, estimate=.pred_glm)
# confusion matrix for decision tree:
results <- stack_test %>% 
  bind_cols(predict(stack_tree, stack_test) %>% 
              rename(.pred_tree=.pred_class))
# confusion matrix for decision tree:
results %>% 
  conf_mat(truth=remote, estimate=.pred_tree)
# confusion matrix for random forest:
results <- stack_test %>% 
  bind_cols(predict(stack_forest, stack_test) %>% 
              rename(.pred_forest=.pred_class))
# confusion matrix for random forest:
results %>% 
  conf_mat(truth=remote, estimate=.pred_forest)
```


# Classification Model Metrics:

The conf_mat() function is helpful but we may wish to store other 
performance metrics in a data frame-friendly form. The yardstick 
package is built to handle such needs. Overall accuracy may be a 
good metric for our models:
```{r}
results <- stack_test %>% 
  bind_cols(predict(stack_tree, stack_test) %>% 
              rename(.pred_tree=.pred_class)) %>% 
             bind_cols(predict(stack_glm, stack_test) %>% 
              rename(.pred_glm=.pred_class)) %>% 
             bind_cols(predict(stack_forest, stack_test) %>% 
              rename(.pred_forest=.pred_class))

## Calculate accuracy:
accuracy(results, truth=remote, estimate=.pred_glm)
accuracy(results, truth=remote, estimate=.pred_tree)
accuracy(results,truth=remote,estimate=.pred_forest)

## Calculate positive predict value:
ppv(results, truth=remote, estimate=.pred_glm)
ppv(results, truth=remote, estimate=.pred_tree)
ppv(results,truth=remote,estimate=.pred_forest)
```
```{r}
set.seed(234)
val_set <- validation_split(stack_train,strata=remote,prop=0.8)
forest_spec <-rand_forest() %>% 
  set_engine("randomForest") %>% 
  set_mode("classification") %>% 
  set_args(mtry=tune(),min_n=tune(),tress=1000)
## start workflow (recipe only)
rf_wf <- workflow() %>% 
  add_recipe(stack_recipe) %>% 
  add_model(forest_spec)
rf_res <- 
 rf_wf %>% 
  tune_grid(val_set,
            grid=25,
            control=control_grid(save_pred=TRUE),
            metrics=metric_set(roc_auc))
# top 5 random forest models:
rf_res %>% 
  show_best(metric="roc_auc")
rf_res %>% 
  collect_predictions()
autoplot(rf_res)
rf_best <- rf_res %>% 
  select_best(metric="roc_auc")
rf_best
```

Filter the predictions for only out best random forest model, we can
use the parameters argument and pass it our tibble with the best 
hyper parameter values from tuning which we called rf_best:
```{r}
rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters=rf_best) %>% 
  roc_curve(remote,.pred_Remote) %>% 
  mutate(model="Random Forest")
rf_auc
```

Plot the ROC curve for our random forest model:
```{r}
rf_auc %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)+
  theme_light()
```

# The last fit

Lets fit the final random forest model. To do so we take our best 
hyper parameter values from our random forest model. When we set 
the engine we add a new argument: importance:"impurity". This 
will provide variable importance score for this last model, 
which gives some insight into which predictors drive model performance.
```{r}
# The last model:
last_rf_mod <- rand_forest(mtry=3, min_n=19,trees=1000) %>% 
  set_engine("randomForest", importance="impurity") %>% 
  set_mode("classification")
# Last workflow:
last_rf_workflow <- 
  rf_wf %>% 
  update_model(last_rf_mod)
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(stack_split)
```


